# Constraints on Self Replicating Robots to Solve Climate Change
by Sven Nilsen, 2017

I made a spreadsheet calculator that [computes the constraints on self replicating robots](https://github.com/advancedresearch/environmental_mega_robotic_systems/blob/master/spreadsheets/self-replication-constraints.pdf).
I will flesh this out when I learn more about the physical constraints.

The obvious problem with solving climate change using self replicating robots,
is that if something goes wrong and you can not stop the robots from replicating,
then you will run out of space on earth's surface very quickly.
One way to make self replicating robots a little safer,
is to use wireless signals to control them remotely,
and hard wire them to not reproduce beyond a low number of generations.

At this moment, it is very uncertain what the real constraints are, so I inserted some rules-of-thumb numbers:

- Less than 5 years to finish replication
- Less than 100 billion agent units
- More than 65-70 days of expected lifetime per unit
- Less than 40 days of replication cycle
- More than 85% success rate of replication
- More than 90% success rate of reuse after a period of replication

Before you decide to use self replicating robots,
you must design a system that is provable safe within these constraints
in addition to solving climate change.

If you have a working design and you can relax some of the constraints,
then it is possible to trade factors,
for example fewer agent units with a lower success rate of replication.

The numbers I chose was based on the assumption that you do not want to use self replicating robots,
unless it is already too late to do anything else, so 5 years of scaling up sounds reasonable to me.
A short period of time, but not so short that it makes scaling up impossible.

The earth is very big, so if you are building something that a single person can build,
you will probably need hundreds of millions, if not billions.
This is because greenhouse gases are spread out over a great volume at low concentrations,
so a lot of air needs to flow through the system.
The larger systems you build, the smaller amounts of them are needed.
I think 100 billion agent units is a good number, because if it is higher, like a trillion units,
then I imagine it gets very hard to keep the system under control.
On the other hand, if your system can scale up even more, then whether it is a billion or a trillion
is not the biggest problem.

The expected lifetime of 65-70 days is set such that there is a 50% chance
that a unit will survive that amount of replication periods.

A replication period on 40 days means that a new robot must be built by a robot
within that time, including transport to a location to start building more robots.
One of the biggest challenges here is to get raw materials and do the assembling fast enough.

The growth rate of the system is the sum of the success rate of replication (85%) and
the success rate of reuse (90%).
This determines how fast the system grows in size.

Ideally, you should make a robot that can self replicate at open sea water,
perhaps by pumping up cold water to use as energy source.
The infrastructure on land will likely deterioate when civilization collapses.
Sea water contains a lot of magnesium, which you can burn in frozen CO2 to produce graphene.
Using a 3D printer, you can then create a material that is stronger than steel and has 10% of the weight.

Remember to not only capture CO2, but also counter-act ocean acidification!
Perhaps is possible to to produce hydrogen as a portable energy source,
while swifting down bicarbonate into the ocean.
Although, you should research the environmental damage before you make any final decisions, doh!

More ideas can be found [here](https://github.com/advancedresearch/environmental_mega_robotic_systems/issues),
labeled with "Information".
